== [[HiveClientImpl]] HiveClientImpl -- The One and Only HiveClient

`HiveClientImpl` is the only available link:spark-sql-HiveClient.adoc[HiveClient] in Spark SQL that does/uses...FIXME

`HiveClientImpl` is <<creating-instance, created>> exclusively when `IsolatedClientLoader` is requested to create a `HiveClient`.

When created `HiveClientImpl` is given the location of the default database for the Hive metastore warehouse (aka <<warehouseDir, warehouseDir>>) which is the value of *hive.metastore.warehouse.dir* Hive-specific configuration property (in a Hadoop configuration).

NOTE: The location of the default database for the Hive metastore warehouse is `/user/hive/warehouse` by default.

NOTE: A Hive metastore warehouse (aka `spark-warehouse`) is to persist tables while a Hive metastore (aka `metastore_db`) is to manage the metadata of the persistent relational entities, e.g. databases, tables, columns, partitions.

NOTE: You may be interested in https://issues.apache.org/jira/browse/SPARK-19664[SPARK-19664 put 'hive.metastore.warehouse.dir' in hadoopConf place] if you use Spark before 2.1 (which you should not really as it is not supported anymore).

CAUTION: FIXME How is `hive.metastore.warehouse.dir` related to `spark.sql.warehouse.dir`? `SharedState.warehousePath`? Review https://github.com/apache/spark/pull/16996/files

CAUTION: FIXME How is this Hadoop configuration created?

=== [[getPartitions]] `getPartitions` Method

[source, scala]
----
getPartitions(
  table: CatalogTable,
  spec: Option[TablePartitionSpec]): Seq[CatalogTablePartition]
----

NOTE: `getPartitions` is part of link:spark-sql-HiveClient.adoc#getPartitions[HiveClient Contract] to...FIXME.

`getPartitions`...FIXME

=== [[getPartitionsByFilter]] `getPartitionsByFilter` Method

[source, scala]
----
getPartitionsByFilter(
  table: CatalogTable,
  predicates: Seq[Expression]): Seq[CatalogTablePartition]
----

NOTE: `getPartitionsByFilter` is part of link:spark-sql-HiveClient.adoc#getPartitionsByFilter[HiveClient Contract] to...FIXME.

`getPartitionsByFilter`...FIXME

=== [[getPartitionOption]] `getPartitionOption` Method

[source, scala]
----
getPartitionOption(
  table: CatalogTable,
  spec: TablePartitionSpec): Option[CatalogTablePartition]
----

NOTE: `getPartitionOption` is part of link:spark-sql-HiveClient.adoc#getPartitionOption[HiveClient Contract] to...FIXME.

`getPartitionOption`...FIXME

=== [[getTableOption]] Getting Table Metadata -- `getTableOption` Method

[source, scala]
----
def getTableOption(dbName: String, tableName: String): Option[CatalogTable]
----

NOTE: `getTableOption` is a part of link:spark-sql-HiveClient.adoc#getTableOption[HiveClient Contract] to...FIXME.

`getTableOption`...FIXME

=== [[creating-instance]] Creating HiveClientImpl Instance

`HiveClientImpl` takes the following when created:

* [[version]] `HiveVersion`
* [[warehouseDir]] Location of the default database for the Hive metastore warehouse if defined (aka `warehouseDir`)
* [[sparkConf]] `SparkConf`
* [[hadoopConf]] Hadoop configuration
* [[extraConfig]] Extra configuration
* [[initClassLoader]] Initial `ClassLoader`
* [[clientLoader]] `IsolatedClientLoader`

`HiveClientImpl` initializes the <<internal-registries, internal registries and counters>>.

=== [[readHiveStats]] Creating Table Statistics from Hive's Table or Partition Parameters -- `readHiveStats` Internal Method

[source, scala]
----
readHiveStats(properties: Map[String, String]): Option[CatalogStatistics]
----

`readHiveStats` creates a link:spark-sql-CatalogStatistics.adoc#creating-instance[CatalogStatistics] from the input Hive table or partition parameters (if available and greater than 0).

.Table Statistics and Hive Parameters
[cols="1,2",options="header",width="100%"]
|===
| Hive Parameter
| Table Statistics

| `totalSize`
| link:spark-sql-CatalogStatistics.adoc#sizeInBytes[sizeInBytes]

| `rawDataSize`
| link:spark-sql-CatalogStatistics.adoc#sizeInBytes[sizeInBytes]

| `numRows`
| link:spark-sql-CatalogStatistics.adoc#rowCount[rowCount]
|===

NOTE: `totalSize` Hive parameter has a higher precedence over `rawDataSize` for link:spark-sql-CatalogStatistics.adoc#sizeInBytes[sizeInBytes] table statistic.

NOTE: `readHiveStats` is used when `HiveClientImpl` is requested for the metadata of a <<getTableOption, table>> or <<fromHivePartition, table partition>>.

=== [[fromHivePartition]] Retrieving Table Partition Metadata (Converting Table Partition Metadata from Hive Format to Spark SQL Format) -- `fromHivePartition` Method

[source, scala]
----
fromHivePartition(hp: HivePartition): CatalogTablePartition
----

`fromHivePartition` simply creates a link:spark-sql-CatalogTablePartition.adoc#creating-instance[CatalogTablePartition] with the following:

1. link:spark-sql-CatalogTablePartition.adoc#spec[spec] from Hive's link:++http://hive.apache.org/javadocs/r2.3.2/api/org/apache/hadoop/hive/ql/metadata/Partition.html#getSpec--++[Partition.getSpec] if available

1. link:spark-sql-CatalogTablePartition.adoc#storage[storage] from Hive's http://hive.apache.org/javadocs/r2.3.2/api/org/apache/hadoop/hive/metastore/api/StorageDescriptor.html[StorageDescriptor] of the table partition

1. link:spark-sql-CatalogTablePartition.adoc#parameters[parameters] from Hive's link:++http://hive.apache.org/javadocs/r2.3.2/api/org/apache/hadoop/hive/ql/metadata/Partition.html#getParameters--++[Partition.getParameters] if available

1. link:spark-sql-CatalogTablePartition.adoc#stats[stats] from Hive's link:++http://hive.apache.org/javadocs/r2.3.2/api/org/apache/hadoop/hive/ql/metadata/Partition.html#getParameters--++[Partition.getParameters] if available and <<readHiveStats, converted to table statistics format>>

NOTE: `fromHivePartition` is used when `HiveClientImpl` is requested for <<getPartitionOption, getPartitionOption>>, <<getPartitions, getPartitions>> and <<getPartitionsByFilter, getPartitionsByFilter>>.
