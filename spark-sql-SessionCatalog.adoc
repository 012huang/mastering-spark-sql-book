== [[SessionCatalog]] SessionCatalog -- Session-Scoped Catalog of Relational Entities

`SessionCatalog` is the catalog of session-scoped relational temporary and permanent relational entities, i.e. databases, tables, temporary views, partitions, and functions.

`SessionCatalog` uses <<externalCatalog, ExternalCatalog>> for the metastore of permanent relational entities only, i.e. databases, tables, partitions, and functions.

You can access the `SessionCatalog` (in a `SparkSession`) through link:spark-sql-SessionState.adoc#catalog[SessionState].

[source, scala]
----
scala> spark.version
res0: String = 2.3.0-SNAPSHOT

scala> :type spark.sessionState.catalog
org.apache.spark.sql.catalyst.catalog.SessionCatalog
----

NOTE: `SessionCatalog` is a layer over <<externalCatalog, ExternalCatalog>> in a link:spark-sql-SparkSession.adoc#sessionState[SparkSession] which allows for different metastores (e.g. in-memory, hive1, hive2) to be used in a single Spark SQL application.

`SessionCatalog` is <<creating-instance, created>> when `SessionState` sets link:spark-sql-SessionState.adoc#catalog[catalog].

[[internal-registries]]
.SessionCatalog's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[tempTables]] `tempTables`
| FIXME

Used when...FIXME

| [[currentDb]] `currentDb`
| FIXME

Used when...FIXME

| [[functionResourceLoader]] `functionResourceLoader`
| FIXME

Used when...FIXME

| [[tableRelationCache]] `tableRelationCache`
| A cache of fully-qualified table names to link:spark-sql-LogicalPlan.adoc[table relation plans] (i.e. `LogicalPlan`).

Used when `SessionCatalog` <<refreshTable, refreshes a table>>
|===

=== [[getTempViewOrPermanentTableMetadata]] `getTempViewOrPermanentTableMetadata` Method

[source, scala]
----
getTempViewOrPermanentTableMetadata(name: TableIdentifier): CatalogTable
----

`getTempViewOrPermanentTableMetadata`...FIXME

NOTE: `getTempViewOrPermanentTableMetadata` is used when...FIXME

=== [[alterPartitions]] `alterPartitions` Method

[source, scala]
----
alterPartitions(tableName: TableIdentifier, parts: Seq[CatalogTablePartition]): Unit
----

`alterPartitions`...FIXME

NOTE: `alterPartitions` is used when...FIXME

=== [[getTableMetadata]] `getTableMetadata` Method

[source, scala]
----
getTableMetadata(name: TableIdentifier): CatalogTable
----

`getTableMetadata`...FIXME

NOTE: `getTableMetadata` is used when...FIXME

=== [[listPartitions]] `listPartitions` Method

[source, scala]
----
listPartitions(
  tableName: TableIdentifier,
  partialSpec: Option[TablePartitionSpec] = None): Seq[CatalogTablePartition]
----

`listPartitions`...FIXME

NOTE: `listPartitions` is used when...FIXME

=== [[alterTableStats]] Altering Table Statistics in Metastore (and Invalidating Internal Cache) -- `alterTableStats` Method

[source, scala]
----
alterTableStats(identifier: TableIdentifier, newStats: Option[CatalogStatistics]): Unit
----

`alterTableStats` requests <<externalCatalog, ExternalCatalog>> to link:spark-sql-ExternalCatalog.adoc#alterTableStats[alterTableStats] and then <<refreshTable, invalidates the table relation cache>>.

`alterTableStats` reports a `NoSuchDatabaseException` if the <<databaseExists, database does not exist>>.

`alterTableStats` reports a `NoSuchTableException` if the <<tableExists, table does not exist>>.

[NOTE]
====
`alterTableStats` is used by the logical commands (when they are executed):

1. link:spark-sql-LogicalPlan-AnalyzeTableCommand.adoc#run[AnalyzeTableCommand], link:spark-sql-LogicalPlan-AnalyzeColumnCommand.adoc#run[AnalyzeColumnCommand], `AlterTableAddPartitionCommand`, `TruncateTableCommand`

1. (*indirectly* through `CommandUtils` when requested for link:spark-sql-CommandUtils.adoc#updateTableStats[updating existing table statistics]) `InsertIntoHiveTable`, `InsertIntoHadoopFsRelationCommand`, `AlterTableDropPartitionCommand`, `AlterTableSetLocationCommand` and `LoadDataCommand`
====

=== [[tableExists]] `tableExists` Method

[source, scala]
----
tableExists(name: TableIdentifier): Boolean
----

`tableExists`...FIXME

NOTE: `tableExists` is used when...FIXME

=== [[databaseExists]] `databaseExists` Method

[source, scala]
----
databaseExists(db: String): Boolean
----

`databaseExists`...FIXME

NOTE: `databaseExists` is used when...FIXME

=== [[functionExists]] `functionExists` Method

CAUTION: FIXME

[NOTE]
====
`functionExists` is used in:

* link:spark-sql-Analyzer.adoc#LookupFunctions[LookupFunctions] logical evaluation rule (to make sure that `UnresolvedFunction` can be resolved, i.e. is registered with `SessionCatalog`)
* `CatalogImpl` to link:spark-sql-CatalogImpl.adoc#functionExists[check if a function exists in a database]
* ...
====

=== [[listFunctions]] `listFunctions` Method

CAUTION: FIXME

=== [[refreshTable]] Invalidating Table Relation Cache (aka Refreshing Table) -- `refreshTable` Method

[source, scala]
----
refreshTable(name: TableIdentifier): Unit
----

`refreshTable`...FIXME

NOTE: `refreshTable` is used when...FIXME

=== [[createTempFunction]] `createTempFunction` Method

CAUTION: FIXME

=== [[loadFunctionResources]] `loadFunctionResources` Method

CAUTION: FIXME

=== [[alterTempViewDefinition]] `alterTempViewDefinition` Method

[source, scala]
----
alterTempViewDefinition(name: TableIdentifier, viewDefinition: LogicalPlan): Boolean
----

`alterTempViewDefinition` alters the temporary view by <<createTempView, updating an in-memory temporary table>> (when a database is not specified and the table has already been registered) or a global temporary table (when a database is specified and it is for global temporary tables).

NOTE: "Temporary table" and "temporary view" are synonyms.

`alterTempViewDefinition` returns `true` when an update could be executed and finished successfully.

=== [[createTempView]] `createTempView` Method

CAUTION: FIXME

=== [[createGlobalTempView]] `createGlobalTempView` Method

CAUTION: FIXME

=== [[createTable]] `createTable` Method

CAUTION: FIXME

=== [[alterTable]] `alterTable` Method

CAUTION: FIXME

=== [[creating-instance]] Creating SessionCatalog Instance

`SessionCatalog` takes the following when created:

* [[externalCatalog]] link:spark-sql-ExternalCatalog.adoc[ExternalCatalog]
* [[globalTempViewManager]] `GlobalTempViewManager`
* [[functionResourceLoader]] `FunctionResourceLoader`
* [[functionRegistry]] link:spark-sql-FunctionRegistry.adoc[FunctionRegistry]
* [[conf]] link:spark-sql-CatalystConf.adoc[CatalystConf]
* [[hadoopConf]] Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[Configuration]
* [[parser]] link:spark-sql-ParserInterface.adoc[ParserInterface]

`SessionCatalog` initializes the <<internal-registries, internal registries and counters>>.

=== [[lookupFunction]] Finding Function by Name (Using FunctionRegistry) -- `lookupFunction` Method

[source, scala]
----
lookupFunction(
  name: FunctionIdentifier,
  children: Seq[Expression]): Expression
----

`lookupFunction` finds a function by `name`.

For a function with no database defined that exists in <<functionRegistry, FunctionRegistry>>, `lookupFunction` requests `FunctionRegistry` to link:spark-sql-FunctionRegistry.adoc#lookupFunction[find the function] (by its unqualified name, i.e. with no database).

If the `name` function has the database defined or does not exist in `FunctionRegistry`, `lookupFunction` uses the fully-qualified function `name` to check if the function exists in <<functionRegistry, FunctionRegistry>> (by its fully-qualified name, i.e. with a database).

For other cases, `lookupFunction` requests <<externalCatalog, ExternalCatalog>> to find the function and <<loadFunctionResources, loads its resources>>. It then <<createTempFunction, creates a corresponding temporary function>> and link:spark-sql-FunctionRegistry.adoc#lookupFunction[looks up the function] again.

NOTE: `lookupFunction` is used exclusively when `Analyzer` link:spark-sql-Analyzer.adoc#ResolveFunctions[resolves functions].
