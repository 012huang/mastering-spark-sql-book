== Cost-Based Optimization (CBO)

*Cost-Based Optimization* (aka *CBO Optimizer*) is an optimization technique in Spark SQL that uses <<statistics, statistics>> to determine the most efficient way of executing a query.

Cost-Based Optimization uses <<optimizations, custom logical optimization rules>> that transform the logical plan of a structured query based on the statistics.

=== [[statistics]] Statistics

The (cost) statistics are as follows:

1. [[total-size-stat]] *Total size* (in bytes) of a link:spark-sql-LogicalPlan-AnalyzeTableCommand.adoc[table] or link:spark-sql-LogicalPlan-AnalyzePartitionCommand.adoc[table partitions]
1. [[row-count-stat]] *Row count* of a link:spark-sql-LogicalPlan-AnalyzeTableCommand.adoc[table] or link:spark-sql-LogicalPlan-AnalyzePartitionCommand.adoc[table partitions]

=== [[spark.sql.cbo.enabled]] spark.sql.cbo.enabled Spark SQL Property

Cost-Based Optimization is enabled by link:spark-sql-SQLConf.adoc#spark.sql.cbo.enabled[spark.sql.cbo.enabled] property (which is disabled by default).

TIP: Use link:spark-sql-SQLConf.adoc#cboEnabled[SQLConf.cboEnabled] to access the current value of `spark.sql.cbo.enabled` property.

[source, scala]
----
// CBO is disabled by default
val sqlConf = spark.sessionState.conf
scala> println(sqlConf.cboEnabled)
false

// Create a new SparkSession with CBO enabled
// You could spark-submit -c spark.sql.cbo.enabled=true
val sparkCboEnabled = spark.newSession
import org.apache.spark.sql.internal.SQLConf.CBO_ENABLED
sparkCboEnabled.conf.set(CBO_ENABLED.key, true)
val isCboEnabled = sparkCboEnabled.conf.get(CBO_ENABLED.key)
println(s"Is CBO enabled? $isCboEnabled")
----

NOTE: CBO is disabled explicitly in Spark Structured Streaming.

=== [[ANALYZE-TABLE]] ANALYZE TABLE SQL Command

Cost-Based Optimization uses the statistics computed and stored in a metastore using link:spark-sql-SparkSqlAstBuilder.adoc#ANALYZE-TABLE[ANALYZE TABLE] SQL command.

[[NOSCAN]]
```
ANALYZE TABLE tableIdentifier partitionSpec?
COMPUTE STATISTICS (NOSCAN | FOR COLUMNS identifierSeq)?
```

Depending on the variant, `ANALYZE TABLE` computes different <<statistics, statistics>>, e.g. total size or row count of a table.

1. `ANALYZE TABLE` with `PARTITION` specification and no `FOR COLUMNS` clause
1. `ANALYZE TABLE` with no `PARTITION` specification and `FOR COLUMNS` clause
1. `ANALYZE TABLE` with `FOR COLUMNS` clause and no `PARTITION` specification

[NOTE]
====
`ANALYZE TABLE` with `PARTITION` specification and `FOR COLUMNS` clause is incorrect.

```
// !!! INCORRECT !!!
ANALYZE TABLE t1 PARTITION (p1, p2) COMPUTE STATISTICS FOR COLUMNS id, p1
```

In such a case, `SparkSqlAstBuilder` reports a WARN message to the logs and simply ignores the partition specification.

```
WARN Partition specification is ignored when collecting column statistics: [partitionSpec]
```
====

=== [[optimizations]] Cost-Based Optimizations

The link:spark-sql-Optimizer.adoc[rule-based Spark Optimizer] comes with rules that are executed when cost-based optimization is <<spark.sql.cbo.enabled, enabled>>.

1. link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder] logical optimization rule for join reordering
